# ğŸ§  Transformer Architecture Notes

These notes summarize some of the most **useful references and materials** Iâ€™ve shared during our paper review sessions on the *Transformer* architecture.  
Theyâ€™re meant as a **living document** â€” part study guide, part annotated reference list â€” combining both theoretical and practical resources that I personally find valuable when explaining or revisiting transformers.
---

## ğŸ§ Podcast

**[SuperDataScience â€” Technical Intro to Transformers and LLMs (Ep. 747, with Kirill Eremenko)](https://www.superdatascience.com/podcast/sds-747-technical-intro-to-transformers-and-llms-with-kirill-eremenko)**  
> *One of my personal favorites.*  
Kirill walks through the **transformer architecture** step-by-step, including the now-famous *â€œfire storyâ€* analogy (the same one I referenced in our discussion).  
If you prefer a narrative and conceptual flow before diving into code, this episode nails it.

---

## ğŸ“º YouTube

**[Transformers Explained Visually](https://www.youtube.com/watch?v=eMlx5fFNoYc&t=217s)**  
A **super intuitive visual breakdown** of the transformer architecture.  
This is one of those videos I keep revisiting â€” it helps solidify whatâ€™s happening at every step of the attention mechanism and how data flows through the layers.  
Perfect if you like mental models over equations.

---

## ğŸ§© Interactive Tool

**[Transformer Explainer â€” by PoloClub](https://poloclub.github.io/transformer-explainer/)**  
A **visual and interactive web app** that lets you explore how each component behaves: multi-head attention, residual connections, layer normalization, etc.  
Extremely useful to *see* whatâ€™s going on internally.

---

## ğŸ“˜ Articles

**[The Illustrated Transformer â€” by Jay Alammar](https://jalammar.github.io/illustrated-transformer/)**  
Still one of the best conceptual resources available.  
Jayâ€™s post connects the math, the structure, and the intuition beautifully.  
Some of the screenshots I used in my notes come from here (annotated based on our own discussions).

---

## ğŸ“„ Papers

**[Attention Is All You Need â€” Vaswani et al., 2017](https://arxiv.org/pdf/1706.03762)**  
The foundational paper.  
Even after years of model evolution (BERT, GPT, ViT, etc.), itâ€™s still *the* place to start if you want to understand **attention as a core mechanism for sequence modeling**.

---

## ğŸ› ï¸ Build It From Scratch

Here are a few implementation-oriented references to bridge theory and code:

- **[Build Your Own Transformer From Scratch Using PyTorch](https://medium.com/data-science/build-your-own-transformer-from-scratch-using-pytorch-84c850470dcb)**  
  A hands-on tutorial walking you through building the transformer from the ground up (embeddings, attention modules, positional encodings, etc.). Great for bridging intuition and implementation.

- **[MayukhSobo / Transformer (GitHub)](https://github.com/MayukhSobo/Transformer)**  
  An â€œhonest implementationâ€ of a full encoder-decoder transformer, with modular components (self-attention, cross-attention, residual + norm layers), multiple tokenizers, WMT14 integration, and well-documented code.  
  I include this as a companion reference because itâ€™s more production-minded (error handling, logging, modular design) while remaining readable. Use it to compare how youâ€™d structure real-world code vs tutorial-level implementations.

---

## ğŸ“š Deep Learning Reference Book

**[Deep Learning: Foundations and Concepts â€” Christopher M. Bishop & Hugh Bishop](https://www.bishopbook.com/)**  
This book goes beyond surface mechanics and helps you **really internalize** foundational ML / DL principles.  
Itâ€™s not just about architectures â€” it covers probability, optimization, theory, and enduring concepts (including transformers) in a way that still feels fresh.  


---

## ğŸ—’ï¸ Slides and Notes

[**My Annotated Slides**](#)  
These slides are mostly **screenshots and notes** from videos, talks, and our internal sessions.  
Theyâ€™re annotated in real time during the discussions, so some context might be missing â€” use them as visual guides rather than full explanations.

- [Download Presentation Slides](../doc/Attention_Notes.pptx)
- [Download Hand Notes](../doc/AttentionNotes.pdf)